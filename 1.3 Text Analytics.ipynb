{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize #import libraries \n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import word_tokenize, sent_tokenize \n",
    "import pandas as pd\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = open(\"analytics.txt\") #read txt file\n",
    "txt = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): #tokenize words\n",
    "    return [word.lower() for word in regexp_tokenize(text, '\\w+')]\n",
    "\n",
    "for word in tokenize(txt):  #frequency of each word type in the paragraph.\n",
    "    fdist[word.lower()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = fdist.N()\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. What is the probability of the word “data” occurring in each line ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent)))\n",
    "              for sent in sent_tokenize(txt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['as', 'a', 'term', ',', 'data', 'analytics', 'predominantly', 'refers', 'to', 'an', 'assortment', 'of', 'applications', ',', 'from', 'basic', 'business', 'intelligence', '(', 'bi', ')', ',', 'reporting', 'and', 'online', 'analytical', 'processing', '(', 'olap', ')', 'to', 'various', 'forms', 'of', 'advanced', 'analytics', '.'], ['in', 'that', 'sense', ',', 'it', \"'s\", 'similar', 'in', 'nature', 'to', 'business', 'analytics', ',', 'another', 'umbrella', 'term', 'for', 'approaches', 'to', 'analyzing', 'data', '--', 'with', 'the', 'difference', 'that', 'the', 'latter', 'is', 'oriented', 'to', 'business', 'uses', ',', 'while', 'data', 'analytics', 'has', 'a', 'broader', 'focus', '.'], ['the', 'expansive', 'view', 'of', 'the', 'term', 'is', \"n't\", 'universal', ',', 'though', ':', 'in', 'some', 'cases', ',', 'people', 'use', 'data', 'analytics', 'specifically', 'to', 'mean', 'advanced', 'analytics', ',', 'treating', 'bi', 'as', 'a', 'separate', 'category', '.'], ['data', 'analytics', 'initiatives', 'can', 'help', 'businesses', 'increase', 'revenues', ',', 'improve', 'operational', 'efficiency', ',', 'optimize', 'marketing', 'campaigns', 'and', 'customer', 'service', 'efforts', ',', 'respond', 'more', 'quickly', 'to', 'emerging', 'market', 'trends', 'and', 'gain', 'a', 'competitive', 'edge', 'over', 'rivals', '--', 'all', 'with', 'the', 'ultimate', 'goal', 'of', 'boosting', 'business', 'performance', '.'], ['depending', 'on', 'the', 'particular', 'application', ',', 'the', 'data', 'that', \"'s\", 'analyzed', 'can', 'consist', 'of', 'either', 'historical', 'records', 'or', 'new', 'information', 'that', 'has', 'been', 'processed', 'for', 'real-time', 'analytics', 'uses', '.'], ['in', 'addition', ',', 'it', 'can', 'come', 'from', 'a', 'mix', 'of', 'internal', 'systems', 'and', 'external', 'data', 'sources', '.'], ['at', 'a', 'high', 'level', ',', 'data', 'analytics', 'methodologies', 'include', 'exploratory', 'data', 'analysis', '(', 'eda', ')', ',', 'which', 'aims', 'to', 'find', 'patterns', 'and', 'relationships', 'in', 'data', ',', 'and', 'confirmatory', 'data', 'analysis', '(', 'cda', ')', ',', 'which', 'applies', 'statistical', 'techniques', 'to', 'determine', 'whether', 'hypotheses', 'about', 'a', 'data', 'set', 'are', 'true', 'or', 'false', '.'], ['eda', 'is', 'often', 'compared', 'to', 'detective', 'work', ',', 'while', 'cda', 'is', 'akin', 'to', 'the', 'work', 'of', 'a', 'judge', 'or', 'jury', 'during', 'a', 'court', 'trial', '--', 'a', 'distinction', 'first', 'drawn', 'by', 'statistician', 'john', 'w.', 'tukey', 'in', 'his', '1977', 'book', 'exploratory', 'data', 'analysis', '.'], ['data', 'analytics', 'can', 'also', 'be', 'separated', 'into', 'quantitative', 'data', 'analysis', 'and', 'qualitative', 'data', 'analysis', '.'], ['the', 'former', 'involves', 'analysis', 'of', 'numerical', 'data', 'with', 'quantifiable', 'variables', 'that', 'can', 'be', 'compared', 'or', 'measured', 'statistically', '.'], ['the', 'qualitative', 'approach', 'is', 'more', 'interpretive', '--', 'it', 'focuses', 'on', 'understanding', 'the', 'content', 'of', 'non-numerical', 'data', 'like', 'text', ',', 'images', ',', 'audio', 'and', 'video', ',', 'including', 'common', 'phrases', ',', 'themes', 'and', 'points', 'of', 'view', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line: 1 - 0.3125\n",
      "Line: 2 - 0.625\n",
      "Line: 3 - 0.3125\n",
      "Line: 4 - 0.3125\n",
      "Line: 5 - 0.3125\n",
      "Line: 6 - 0.3125\n",
      "Line: 7 - 1.5625\n",
      "Line: 8 - 0.3125\n",
      "Line: 9 - 0.9375\n",
      "Line: 10 - 0.3125\n",
      "Line: 11 - 0.3125\n"
     ]
    }
   ],
   "source": [
    "#probability of word data in each line\n",
    "import nltk \n",
    "Line = 0\n",
    "for i in tokenized_text:\n",
    "    counts  = 0\n",
    "    Line +=1\n",
    "    for words in i:\n",
    "        Total =+1\n",
    "        if words == \"data\":\n",
    "            counts +=1\n",
    "\n",
    "   \n",
    "    \n",
    "    print(\"Line:\", Line,\"-\", (counts/total)*100 )\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. What is the distribution of distinct word counts across all the lines ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "Freq = fdist.items() #used to return the list with all dictionary keys with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('as', 2), ('a', 10), ('term', 3), ('data', 18), ('analytics', 10), ('predominantly', 1), ('refers', 1), ('to', 11), ('an', 1), ('assortment', 1), ('of', 10), ('applications', 1), ('from', 2), ('basic', 1), ('business', 4), ('intelligence', 1), ('bi', 2), ('reporting', 1), ('and', 9), ('online', 1), ('analytical', 1), ('processing', 1), ('olap', 1), ('various', 1), ('forms', 1), ('advanced', 2), ('in', 6), ('that', 5), ('sense', 1), ('it', 3), ('s', 2), ('similar', 1), ('nature', 1), ('another', 1), ('umbrella', 1), ('for', 2), ('approaches', 1), ('analyzing', 1), ('with', 3), ('the', 11), ('difference', 1), ('latter', 1), ('is', 4), ('oriented', 1), ('uses', 2), ('while', 2), ('has', 2), ('broader', 1), ('focus', 1), ('expansive', 1), ('view', 2), ('isn', 1), ('t', 1), ('universal', 1), ('though', 1), ('some', 1), ('cases', 1), ('people', 1), ('use', 1), ('specifically', 1), ('mean', 1), ('treating', 1), ('separate', 1), ('category', 1), ('initiatives', 1), ('can', 5), ('help', 1), ('businesses', 1), ('increase', 1), ('revenues', 1), ('improve', 1), ('operational', 1), ('efficiency', 1), ('optimize', 1), ('marketing', 1), ('campaigns', 1), ('customer', 1), ('service', 1), ('efforts', 1), ('respond', 1), ('more', 2), ('quickly', 1), ('emerging', 1), ('market', 1), ('trends', 1), ('gain', 1), ('competitive', 1), ('edge', 1), ('over', 1), ('rivals', 1), ('all', 1), ('ultimate', 1), ('goal', 1), ('boosting', 1), ('performance', 1), ('depending', 1), ('on', 2), ('particular', 1), ('application', 1), ('analyzed', 1), ('consist', 1), ('either', 1), ('historical', 1), ('records', 1), ('or', 4), ('new', 1), ('information', 1), ('been', 1), ('processed', 1), ('real', 1), ('time', 1), ('addition', 1), ('come', 1), ('mix', 1), ('internal', 1), ('systems', 1), ('external', 1), ('sources', 1), ('at', 1), ('high', 1), ('level', 1), ('methodologies', 1), ('include', 1), ('exploratory', 2), ('analysis', 6), ('eda', 2), ('which', 2), ('aims', 1), ('find', 1), ('patterns', 1), ('relationships', 1), ('confirmatory', 1), ('cda', 2), ('applies', 1), ('statistical', 1), ('techniques', 1), ('determine', 1), ('whether', 1), ('hypotheses', 1), ('about', 1), ('set', 1), ('are', 1), ('true', 1), ('false', 1), ('often', 1), ('compared', 2), ('detective', 1), ('work', 2), ('akin', 1), ('judge', 1), ('jury', 1), ('during', 1), ('court', 1), ('trial', 1), ('distinction', 1), ('first', 1), ('drawn', 1), ('by', 1), ('statistician', 1), ('john', 1), ('w', 1), ('tukey', 1), ('his', 1), ('1977', 1), ('book', 1), ('also', 1), ('be', 2), ('separated', 1), ('into', 1), ('quantitative', 1), ('qualitative', 2), ('former', 1), ('involves', 1), ('numerical', 2), ('quantifiable', 1), ('variables', 1), ('measured', 1), ('statistically', 1), ('approach', 1), ('interpretive', 1), ('focuses', 1), ('understanding', 1), ('content', 1), ('non', 1), ('like', 1), ('text', 1), ('images', 1), ('audio', 1), ('video', 1), ('including', 1), ('common', 1), ('phrases', 1), ('themes', 1), ('points', 1)])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame.from_dict(Freq) #convert dictionary to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>term</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>analytics</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>predominantly</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>refers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>to</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>an</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>assortment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>of</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>applications</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>from</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>basic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>business</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>intelligence</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>bi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>reporting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>online</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>analytical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>processing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>olap</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>various</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>forms</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>advanced</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>in</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>that</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sense</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>it</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>book</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>also</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>be</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>separated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>into</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>quantitative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>qualitative</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>former</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>involves</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>numerical</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>quantifiable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>variables</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>measured</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>statistically</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>approach</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>interpretive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>focuses</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>understanding</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>content</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>non</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>like</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>images</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>audio</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>including</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>common</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>phrases</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>themes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>points</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0   1\n",
       "0               as   2\n",
       "1                a  10\n",
       "2             term   3\n",
       "3             data  18\n",
       "4        analytics  10\n",
       "5    predominantly   1\n",
       "6           refers   1\n",
       "7               to  11\n",
       "8               an   1\n",
       "9       assortment   1\n",
       "10              of  10\n",
       "11    applications   1\n",
       "12            from   2\n",
       "13           basic   1\n",
       "14        business   4\n",
       "15    intelligence   1\n",
       "16              bi   2\n",
       "17       reporting   1\n",
       "18             and   9\n",
       "19          online   1\n",
       "20      analytical   1\n",
       "21      processing   1\n",
       "22            olap   1\n",
       "23         various   1\n",
       "24           forms   1\n",
       "25        advanced   2\n",
       "26              in   6\n",
       "27            that   5\n",
       "28           sense   1\n",
       "29              it   3\n",
       "..             ...  ..\n",
       "164           book   1\n",
       "165           also   1\n",
       "166             be   2\n",
       "167      separated   1\n",
       "168           into   1\n",
       "169   quantitative   1\n",
       "170    qualitative   2\n",
       "171         former   1\n",
       "172       involves   1\n",
       "173      numerical   2\n",
       "174   quantifiable   1\n",
       "175      variables   1\n",
       "176       measured   1\n",
       "177  statistically   1\n",
       "178       approach   1\n",
       "179   interpretive   1\n",
       "180        focuses   1\n",
       "181  understanding   1\n",
       "182        content   1\n",
       "183            non   1\n",
       "184           like   1\n",
       "185           text   1\n",
       "186         images   1\n",
       "187          audio   1\n",
       "188          video   1\n",
       "189      including   1\n",
       "190         common   1\n",
       "191        phrases   1\n",
       "192         themes   1\n",
       "193         points   1\n",
       "\n",
       "[194 rows x 2 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. What is the probability of the word “analytics” occurring after the word “data” ?  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.lm import MLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
    "\n",
    "model = MLE(n)  # Lets train a 2-grams maximum likelihood estimation model.\n",
    "model.fit(train_data, padded_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.counts[['data']]['analytics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.33'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Probability = model.score('analytics', 'data'.split()) # P('analytics'|'data')\n",
    "format(Probability, '.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
